# Slim Dockerfile with Quantized Model (~3.5GB total)
FROM python:3.10-slim

WORKDIR /app

# Install minimal dependencies
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Copy and install Python requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Use QUANTIZED model (4-bit instead of 16-bit)
# This reduces size from 7.5GB to 2.5GB with minimal quality loss
RUN echo '#!/bin/bash\n\
ollama serve &\n\
sleep 10\n\
# Pull quantized model - MUCH smaller\n\
ollama pull mistral:7b-instruct-q4_0\n\
# Start app\n\
uvicorn main:app --host 0.0.0.0 --port ${PORT:-8000}' > start.sh && \
    chmod +x start.sh

EXPOSE 8000
CMD ["./start.sh"]